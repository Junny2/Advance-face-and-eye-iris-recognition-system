{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This notebook is for identifying the faces\n",
    "This notebook has functionalities for taking pictures and comparing with the know faces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../Models/face_recognition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "\n",
    "# Load the jpg files into numpy arrays\n",
    "biden_image = face_recognition.load_image_file(\"biden.jpg\")\n",
    "obama_image = face_recognition.load_image_file(\"obama.jpg\")\n",
    "unknown_image = face_recognition.load_image_file(\"obama2.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IdentifyFace(image):\n",
    "    unknown_image = face_recognition.load_image_file(image)\n",
    "    \n",
    "    try:\n",
    "        unknown_face_encoding = face_recognition.face_encodings(unknown_image)[0]\n",
    "    except IndexError:\n",
    "        print(\"I wasn't able to locate any faces in at least one of the images. Check the image files. Aborting...\")\n",
    "        return \n",
    "    # loading face images from faces folder for comparision:\n",
    "    \n",
    "    faces = os.listdir(\"../Data/Faces\")\n",
    "    \n",
    "    for face in faces:\n",
    "        face_image = face_recognition.load_image_file(\"../Data/Faces\" + face)\n",
    "        face_encodings = face_recognition.face_encodings(face_image)[0]\n",
    "        \n",
    "\n",
    "        known_faces = [face_encodings]\n",
    "        # results is an array of True/False telling if the unknown face matched anyone in the known_faces array\n",
    "        results = face_recognition.compare_faces(known_faces, unknown_face_encoding)\n",
    "        if results[0] == True:\n",
    "            return face_image\n",
    "    print(\"Is the unknown face a new person that we've never seen before? {}\".format(not True in results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identifying face location from an image\n",
    "image = face_recognition.load_image_file(\"../Data/sampleface.jpg\")\n",
    "face_locations = face_recognition.face_locations(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(400, 707, 862, 245)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing Image \n",
    "image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "cv2.imshow('Image Face',image)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(400, 707, 862, 245)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying face landmarks from a image\n",
    "face_landmarks_list = face_recognition.face_landmarks(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_eye = face_landmarks_list[0]['right_eye']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_eye = face_landmarks_list[0]['left_eye']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(525, 521), (548, 499), (577, 495), (600, 510), (581, 519), (552, 522)],\n",
       " [(332, 519), (355, 502), (384, 504), (409, 525), (383, 527), (353, 527)])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right_eye, left_eye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'extractRectangle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-d4f8faaa8e2d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Making rectangle from right_eye\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mright_rect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextractRectangle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright_eye\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# Drawing rectangle:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrectangle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mright_rect\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mright_rect\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'extractRectangle' is not defined"
     ]
    }
   ],
   "source": [
    "# Making rectangle from right_eye\n",
    "right_rect = extractRectangle(right_eye)\n",
    "# Drawing rectangle:\n",
    "image = cv2.rectangle(image,right_rect[0],right_rect[1],[256,256,0],5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"../Data/sampleface.jpg\")\n",
    "# Drawing left eye:\n",
    "for i in range(len(right_eye) - 1):\n",
    "    p1 = left_eye[i]\n",
    "    p2 = left_eye[i + 1]\n",
    "    img = cv2.line(img ,p1, p2,[256,256,0],1)\n",
    "    \n",
    "img = cv2.line(img ,left_eye[0], left_eye[-1],[256,256,0],1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-8260be6fac21>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_RGB2BGR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;33m(\u001b[0m\u001b[0mx_min\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_min\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_max\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mleft_eye\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0msub\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetRectSubPix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft_eye\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft_eye\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Image Face'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "(x_min, y_min),(x_max, y_max) = left_eye\n",
    "sub = cv2.getRectSubPix(image, left_eye[0], left_eye[-1])\n",
    "cv2.imshow('Image Face',sub)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drawing a boundary to the eyes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# live webcam video\n",
    "# Get a reference to webcam #0 (the default one)\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "# Initialize some variables\n",
    "face_locations = []\n",
    "right_eye = []\n",
    "left_eye = []\n",
    "\n",
    "process_this_frame = True\n",
    "\n",
    "while True:\n",
    "    # Grab a single frame of video\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    # Resize frame of video to 1/4 size for faster face recognition processing\n",
    "    small_frame = cv2.resize(frame, (0, 0), fx=1.5, fy=1.5)\n",
    "\n",
    "    # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n",
    "    img = small_frame[:, :, ::-1]\n",
    "\n",
    "    \n",
    "    # Find all the eye locations:\n",
    "    face_landmarks_list = face_recognition.face_landmarks(img)\n",
    "    \n",
    "    try:\n",
    "        right_eye = face_landmarks_list[0]['right_eye']\n",
    "        for i in range(len(right_eye) - 1):\n",
    "            p1 = right_eye[i]\n",
    "            p2 = right_eye[i + 1]\n",
    "            img = cv2.line(img ,p1, p2,[256,256,0],1)\n",
    "        img = cv2.line(img ,right_eye[0], right_eye[-1],[256,256,0],1)\n",
    "\n",
    "        \n",
    "        left_eye  = face_landmarks_list[0]['left_eye']\n",
    "        for i in range(len(left_eye) - 1):\n",
    "            p1 = left_eye[i]\n",
    "            p2 = left_eye[i + 1]\n",
    "            img = cv2.line(img ,p1, p2,[256,256,0],1)\n",
    "        img = cv2.line(img ,left_eye[0], left_eye[-1],[256,256,0],1)\n",
    "    \n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # Display the resulting image\n",
    "    cv2.imshow('Video', img)\n",
    "\n",
    "    # Hit 'q' on the keyboard to quit!\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release handle to the webcam\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eye identification system"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Steps:\n",
    "    1. Taking and capturing details\n",
    "        Camera is opened\n",
    "        Eye locations are extracted\n",
    "        A rectangular portion is masked\n",
    "        With a button press\n",
    "        The rectangular portion is stored in the data base\n",
    "        \n",
    "    2. Comparing with the existing eye images\n",
    "        The eye image is located and rectagular portion is extracted\n",
    "        It is compared with the existing database\n",
    "        Result is given "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### With Static image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractRectangle(eye_locations):\n",
    "    x_min = min([ x[0] for x in eye_locations])\n",
    "    x_max = max([ x[0] for x in eye_locations])\n",
    "\n",
    "    y_min = min([ x[1] for x in eye_locations])\n",
    "    y_max = max([ x[1] for x in eye_locations])\n",
    "    \n",
    "    return (x_min, y_min),(x_max, y_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting the eye image from the image and storing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the name of the person :priya\n"
     ]
    }
   ],
   "source": [
    "# step 1:\n",
    "# Get a reference to webcam #0 (the default one)\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "right_eye = []\n",
    "left_eye = []\n",
    "\n",
    "# Enter Name: \n",
    "name = input(\"Enter the name of the person :\")\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    small_frame = cv2.resize(frame, (0, 0), fx=1, fy=1)\n",
    "\n",
    "    # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n",
    "    img = small_frame[:, :, ::-1]\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    # Find all the eye locations:\n",
    "    face_landmarks_list = face_recognition.face_landmarks(img)\n",
    "    \n",
    "    # Right eye locations are extracted\n",
    "    right_eye = face_landmarks_list[0]['right_eye']\n",
    "    (x_min, y_min),(x_max, y_max) = extractRectangle(right_eye)\n",
    "    i = 5\n",
    "    right_eye_img = img[y_min  - i :y_max + i , x_min - i:x_max +i ,:]\n",
    "\n",
    "    # img = cv2.rectangle(img ,right_rect[0], right_rect[-1],[256,256,0],1)\n",
    "    left_eye  = face_landmarks_list[0]['left_eye']\n",
    "    (x_min, y_min),(x_max, y_max) = extractRectangle(left_eye)\n",
    "    left_eye_img = img[y_min  - i :y_max + i , x_min - i:x_max +i ,:]\n",
    "         \n",
    "    \n",
    "    # Showing croped image:\n",
    "    cv2.imshow(\"Video\",img)\n",
    "    \n",
    "    # Hit 'q' on the keyboard to quit!\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        # Capture the image and store it\n",
    "        # left eye:\n",
    "        try:\n",
    "            os.mkdir(\"../Data/eye images/\"+ name )\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        cv2.imwrite(\"../Data/eye images/\"+ name + \"/left.jpg\", left_eye_img)\n",
    "        \n",
    "        # right eye:\n",
    "        cv2.imwrite(\"../Data/eye images/\"+ name +\"/right.jpg\", right_eye_img)\n",
    "        \n",
    "        break\n",
    "        \n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from skimage.measure import compare_ssim\n",
    "\n",
    "# Getting comparision\n",
    "def getUser(eye_image, flag = 'left'):\n",
    "    # list of users:\n",
    "    (h,w) = eye_image.shape[:2]\n",
    "    \n",
    "    users = os.listdir(\"../Data/eye images\")\n",
    "    scores = []\n",
    "    for user in users:\n",
    "        img = cv2.imread(\"../Data/eye images/\"+user+\"/left.jpg\")\n",
    "        img = cv2.resize(img, (w,h), interpolation = cv2.INTER_AREA)\n",
    "        \n",
    "        score = compare_ssim(eye_image, img, multichannel=True)\n",
    "        scores.append(0)\n",
    "        \n",
    "    return users[np.argmax(scores)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The user is: \n",
      "yesh\n",
      "yesh\n"
     ]
    }
   ],
   "source": [
    "# step 1:\n",
    "# Get a reference to webcam #0 (the default one)\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "right_eye = []\n",
    "left_eye = []\n",
    "\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    small_frame = cv2.resize(frame, (0, 0), fx=1, fy=1)\n",
    "\n",
    "    # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n",
    "    img = small_frame[:, :, ::-1]\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    # Find all the eye locations:\n",
    "    face_landmarks_list = face_recognition.face_landmarks(img)\n",
    "    \n",
    "    # Right eye locations are extracted\n",
    "    right_eye = face_landmarks_list[0]['right_eye']\n",
    "    (x_min, y_min),(x_max, y_max) = extractRectangle(right_eye)\n",
    "    i = 5\n",
    "    right_eye_img = img[y_min  - i :y_max + i , x_min - i:x_max +i ,:]\n",
    "\n",
    "    # img = cv2.rectangle(img ,right_rect[0], right_rect[-1],[256,256,0],1)\n",
    "    left_eye  = face_landmarks_list[0]['left_eye']\n",
    "    (x_min, y_min),(x_max, y_max) = extractRectangle(left_eye)\n",
    "    left_eye_img = img[y_min  - i :y_max + i , x_min - i:x_max +i ,:]\n",
    "         \n",
    "    \n",
    "    # Showing croped image:\n",
    "    cv2.imshow(\"Video\",img)\n",
    "    \n",
    "    # Hit 'q' on the keyboard to quit!\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        # Capture the image and store it\n",
    "        # left eye:\n",
    "        print(\"The user is: \")\n",
    "        print(getUser(left_eye_img, flag = 'left'))\n",
    "        \n",
    "        # right eye:\n",
    "        print(getUser(right_eye_img, flag = 'right'))\n",
    "        \n",
    "        break\n",
    "        \n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting the eye iris image from the eye image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading eye image:\n",
    "img = cv2.imread(\"../Data/eye images/yeshu/left.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the eye image\n",
    "cv2.imshow('Left eye image',img)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blurring the image:\n",
    "temp_left = cv2.medianBlur( img ,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the bullred image\n",
    "cv2.imshow('Left eye image Blurred',temp_left)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to gray scale image \n",
    "gray = cv2.cvtColor(temp_left, cv2.COLOR_BGR2GRAY)\n",
    "cv2.imshow('Gray image',gray)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[22.5, 10.5,  9.6],\n",
       "        [22.5,  8.5,  8. ],\n",
       "        [20.5, 13.5,  6.9]]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Circular hough transforms:\n",
    "circles = cv2.HoughCircles(gray,cv2.HOUGH_GRADIENT,1,2, param1=1,param2=15,minRadius=5,maxRadius=100)\n",
    "circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting float to int\n",
    "circles = np.uint16(np.around(circles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing circles to the image\n",
    "for i in circles[0,:][:1]:\n",
    "    # draw the outer circle\n",
    "    cv2.circle(img,(i[0],i[1]),i[2],(0,255,0),2)\n",
    "    \n",
    "    # draw the center of the circle\n",
    "    cv2.circle(img,(i[0],i[1]),2,(0,0,255),1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the circular portion of the image:\n",
    "height,width = img.shape[:2]\n",
    "\n",
    "mask = np.zeros((height,width), np.uint8)\n",
    "\n",
    "mask = cv2.circle(mask,(i[0],i[1]),i[2],(255,255,255),thickness=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the circular portion\n",
    "masked_data = cv2.bitwise_and(img, img, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enlaring the masked image:\n",
    "masked_data = cv2.resize(masked_data, (0, 0), fx=2.5, fy=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('masked_data iris image',masked_data)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
